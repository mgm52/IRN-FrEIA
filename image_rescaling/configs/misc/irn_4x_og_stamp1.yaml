# Model
scale: 4 # no. downsampling layers = log2(scale)
inv_per_ds: 8
inv_first_level_extra: 0
inv_final_level_extra: 0
batchnorm: False
img_size: 144
actnorm: False
zerosample: False
clamp: 1.0
clamp_min: -1.0
clamp_tightness: 1.0
sr_mode: False

# Training
batch_size: 16
lambda_recon: 1
lambda_guide: 16
lambda_distr: 1
initial_learning_rate: 0.0002
seed: 10
grad_clipping: 10
grad_value_clipping: null
lr_batch_milestones: [100000, 200000, 300000, 400000]
lr_gamma: 0.5
mean_losses: False
quantize_recon_loss: False
quantize_guide_loss: False # currently not used
y_channel_usage: 0
epochs_between_flip_loss: null
flip_loss_scale: null
stamp_size: 1

# Training output
max_batches: 525000
max_epochs: -1
target_loss: -1
epochs_between_tests: 100
epochs_between_training_log: 1
epochs_between_samples: 5
epochs_between_saves: 10

# Testing
full_size_test_imgs: True
fast_gpu_testing: False # gives slightly inaccurate SSIM but much faster testing