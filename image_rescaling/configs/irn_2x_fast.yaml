# Model
scale: 2 # no. downsampling layers = log2(scale)
inv_per_ds: 8
inv_first_level_extra: 0
inv_final_level_extra: 0
batchnorm: False
img_size: 64
actnorm: False

# Training
batch_size: 32
lambda_recon: 1
lambda_guide: 16
lambda_distr: 1
initial_learning_rate: 0.0002
seed: 10
grad_clipping: 10
grad_value_clipping: null
lr_batch_milestones: [25000, 50000, 75000, 100000]
lr_gamma: 0.5
mean_losses: False
quantize_recon_loss: False
quantize_guide_loss: False # currently not used

# Training output
max_batches: 500000
max_epochs: -1
target_loss: -1
epochs_between_tests: 2
epochs_between_training_log: 0.2
epochs_between_samples: 2
epochs_between_saves: 2

# Testing
full_size_test_imgs: True
fast_gpu_testing: False # gives slightly inaccurate SSIM but much faster testing