# Model
scale: 4 # no. downsampling layers = log2(scale)
inv_per_ds: 8
inv_first_level_extra: 0
inv_final_level_extra: 0
batchnorm: False
img_size: 160
actnorm: False
zerosample: False

# Training
batch_size: 16
lambda_recon: 1
lambda_guide: 16 #0.05
lambda_distr: 1 #0.00001 #<- unstable
initial_learning_rate: 0.00025
seed: 10
grad_clipping: 10
grad_value_clipping: null
lr_batch_milestones: [92000, 200000, 300000, 400000, 450000]
lr_gamma: 0.5
mean_losses: False
quantize_recon_loss: False
quantize_guide_loss: False # currently not used

# Training output
max_batches: 525000
max_epochs: -1
target_loss: -1
epochs_between_tests: 100
epochs_between_training_log: 0.2
epochs_between_samples: 25
epochs_between_saves: 10

# Testing
full_size_test_imgs: True
fast_gpu_testing: False # gives slightly inaccurate SSIM but much faster testing